{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd4b2604-52d9-4494-8258-ba9ef6c407c3",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- scale 512x512 les images avant de les rentrer dans le boxdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0301a81e-edb9-42cf-83fa-bb799cea8688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs import preprocessing as pp\n",
    "from libs.preprocessing.dataset import imageDataset\n",
    "from libs.utils import imageToTensor, crop_imgs, draw_img_boxes\n",
    "from carDetector import carDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb8e279-0dd9-44e5-97b0-512289d32163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a23f8e-87f5-40e2-a348-a8f9dbe8dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aee3d9-e389-4183-9c9a-7cab8bccfbb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "home = \"/home/jovyan/activities_data/hi__paris_2022_hackathon/final_challenge/datasets\"\n",
    "car_path = f\"{home}/car_models_footprint.csv\"\n",
    "annotation_path = f\"{home}/datasets_train/train_annotation/_annotation.csv\"\n",
    "images_path = f\"{home}/datasets_train/train\"\n",
    "car_images_path = f\"{home}/datasets_train/car_models_database_train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fddca3-1355-4cfb-822a-861e96d68f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = pp.create_car_dataset(car_path, annotation_path, label=\"brand\")\n",
    "carDataset = imageDataset(x, images_path)\n",
    "labelEncoder = preprocessing.LabelEncoder().fit(y[\"models\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1eafe6-aec8-4a22-ba46-32265deb73f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_classifier, y_classifier = pp.create_image_dataset(car_images_path)\n",
    "y_classifier_id = labelEncoder.transform(y_classifier[\"label\"])\n",
    "classifierDataset = imageDataset(X_classifier, car_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180ee7f2-956c-48ef-abe3-bce1389a1857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.nn.functional import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03562f55-b363-4370-a102-b55dedc8fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageCollater(x, size=256, augment=True):\n",
    "    \"\"\"\n",
    "        Resize and collate image together\n",
    "        \n",
    "        x: list of images numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    # Resize image\n",
    "    resizer = transforms.Resize(size=size)\n",
    "    \n",
    "    # Random crop\n",
    "    cropper = transforms.CenterCrop((size, size))\n",
    "    \n",
    "    # Random flip\n",
    "    flipper_1 = transforms.RandomHorizontalFlip()\n",
    "    flipper_2 = transforms.RandomVerticalFlip()\n",
    "    \n",
    "    # Random rotate\n",
    "    rotater = transforms.RandomRotation((-90, 90))\n",
    "    \n",
    "    # Application of the operations\n",
    "    tensors_list = []\n",
    "    for image in x:\n",
    "        image = imageToTensor(image)\n",
    "        \n",
    "        # Fixing missing axis :\n",
    "        if image.dim() == 2:\n",
    "            image = image.repeat(3, 1, 1)\n",
    "        \n",
    "        image = resizer(image)\n",
    "        image = cropper(image)\n",
    "        if augment == True:\n",
    "            image = flipper_1(flipper_2(image))\n",
    "            image = rotater(image)        \n",
    "        tensors_list.append(image)\n",
    "        \n",
    "    tensors = torch.stack(tensors_list)\n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202bd088-9d28-4166-ac7c-fa8426f38650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b0233a-c4eb-4ae3-a50a-3986b6c86e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class imageClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "        Given an image, return its class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_classes = 100):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        pretrained = models.efficientnet_b5(pretrained=True)\n",
    "        backbone = nn.Sequential(*list(pretrained.children())[0:2])\n",
    "        for param in backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        self.scaler = transforms.Lambda(lambda x: x/255.)\n",
    "        self.network = nn.Sequential(*[\n",
    "            backbone,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048, n_classes),\n",
    "            nn.Softmax(dim=0)\n",
    "        ])\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=1e-2)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Return the softmax prediction given an image\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.scaler(x)\n",
    "        y_hat = self.network(x)\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        \"\"\"\n",
    "            Fit dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        self.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.detach().item()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "            Return the prediction for a given image\n",
    "        \"\"\"\n",
    "        \n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_hat = self.forward(x)\n",
    "            \n",
    "        return(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50202d92-4409-4657-a8ab-800b2a891e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierDataloader = DataLoader(range(len(X_classifier)), shuffle=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8f87a3-a234-4902-b232-0f2bbfb0365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = imageClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ab56be-5951-42ec-989a-878e9303e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f20b60-3873-4b08-be11-208e075225d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "classifier.to(device)\n",
    "\n",
    "losses = []\n",
    "for i in range(100):\n",
    "    for idx in classifierDataloader:\n",
    "        images = imageCollater(classifierDataset[idx]).to(device)\n",
    "        labels = one_hot(torch.tensor(y_classifier_id[idx]), num_classes=100).to(device).float()\n",
    "\n",
    "        loss = classifier.fit(images, labels)\n",
    "        losses.append(loss)\n",
    "\n",
    "    print(np.array(losses).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59b29aa-45ef-455c-928a-e10281051e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b5\n",
    "\n",
    "efficientnet_model = efficientnet_b5(pretrained=True)\n",
    "\n",
    "class MyEfficientNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyEfficientNet, self).__init__()\n",
    "        self.first = torch.nn.Sequential(*list(efficientnet_model.children())[:-1])\n",
    "        self.dropout = torch.nn.Dropout(p=0.4, inplace=True)\n",
    "        self.classifier = torch.nn.Linear(2048, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first(x)\n",
    "        x = x[:,:,0,0]\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecb8a0b-16fe-4af5-ac6f-bf00cd3c97df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.models import detection\n",
    "\n",
    "\n",
    "\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33095c1f-43bf-4aac-a544-33e50461fc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "importlib.reload(pp)\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28afdefb-8902-4d1c-82e2-36d50f415a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "footprint_path = (\n",
    "    \"~/activities_data/\"\n",
    "    \"hi__paris_2022_hackathon/\"\n",
    "    \"final_challenge/\"\n",
    "    \"datasets/\"\n",
    "    \"car_models_footprint.csv\"\n",
    ")\n",
    "\n",
    "car_path = (\n",
    "    \"~/activities_data/\"\n",
    "    \"hi__paris_2022_hackathon/\"\n",
    "    \"final_challenge/\"\n",
    "    \"datasets/\"\n",
    "    \"datasets_train/\"\n",
    "    \"train_annotation/\"\n",
    "    \"_annotation.csv\"\n",
    ")\n",
    "\n",
    "imgs_path = (\n",
    "    \"activities_data/\"\n",
    "    \"hi__paris_2022_hackathon/\"\n",
    "    \"final_challenge/\"\n",
    "    \"datasets/\"\n",
    "    \"datasets_train/\"\n",
    "    \"train/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22211ad6-9a53-465a-abfd-dfa7fc7f7568",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proportion = 0.7\n",
    "\n",
    "num_train = int(len(img_dataset) * train_proportion)\n",
    "num_test = len(img_dataset) - num_train\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(img_dataset, [num_train, num_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8700fa-472c-4c5d-bdcc-46e97a80d323",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b46d8-79ef-48d3-a8e9-531e22b87c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "models = {\n",
    "    \"frcnn-resnet\": detection.fasterrcnn_resnet50_fpn,\n",
    "    \"frcnn-mobilenet\": detection.fasterrcnn_mobilenet_v3_large_320_fpn,\n",
    "    \"retinanet\": detection.retinanet_resnet50_fpn,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7010613a-8953-4151-8b07-436e146410e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_set = pp.BoxDataset(img_dataset, retinanet, device, tol=4e-1, class_idx=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc056132-dc11-4713-8a5c-a2b2a0c861b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b5\n",
    "\n",
    "efficientnet_model = efficientnet_b5(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6852ed4e-3253-4bfa-8914-79e7cb72d200",
   "metadata": {},
   "outputs": [],
   "source": [
    "myefficientnet = MyEfficientNet().to(device)\n",
    "myefficientnet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b44ee39-03fd-47f7-aeba-4ecbb44f0d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_loader = torch.utils.data.DataLoader(box_set, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8e8fc7-547e-482e-8caf-eecd4f2e0aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(efficientnet_model.parameters(), lr=1e-3)\n",
    "\n",
    "img_size = 256 # 456 efficient net original training size\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.CenterCrop(img_size),\n",
    "])\n",
    "\n",
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a561aec9-6287-458a-8984-ce99d8c65896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "importlib.reload(pp)\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd6c68-689c-4d0b-b7d3-08662de658a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "pbar = tqdm(range(1, n_epochs+1))\n",
    "for epoch in pbar:\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i_batch, ((imgs, boxes), labels) in enumerate(box_loader):\n",
    "\n",
    "        mask = (boxes.sum(dim=1) > 0) & (labels != -1) # on prend les images qui n'ont pas ete predites (0,0,0,0) et dont le label est celui d'une voiture\n",
    "        imgs = imgs[mask].to(device)\n",
    "        boxes = boxes[mask].to(device)\n",
    "        labels = labels[mask].to(device)\n",
    "\n",
    "        if mask.sum() > 0:\n",
    "            # attention : definit la taille des images qui rentrent dans le cnn\n",
    "            imgs = utils.crop_imgs(imgs, boxes, data_transform)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            preds = myefficientnet(imgs)\n",
    "\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                'progress': '{0:.2f}'.format(100*i_batch/len(box_loader)) + '%',\n",
    "                'batch loss': '{0:.2f}'.format(loss.item())\n",
    "            })\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(box_loader)\n",
    "    pbar.set_description('{:.5e}'.format(epoch_loss))\n",
    "\n",
    "    losses.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae8d36-a1c3-4128-b740-9530355cd496",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(myefficientnet, 'my_work/team-03-084-submission-matthieu/Code/trained_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b1916-f689-42eb-840c-41eec4d35507",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9738a4-b039-4b92-a9b0-86a6a00f90af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
